{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "mount_file_id": "1HCuTdj4XCDY2cwgz2t49RY3eJcA58ayq",
      "authorship_tag": "ABX9TyNk4sYxlgQyPFBu3MzmEh+6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/woosansam/-/blob/main/Transformer_%EC%98%81%EC%96%B4_%ED%95%9C%EA%B8%80_%EC%A7%80%EC%A7%80%EC%A7%84%EC%A7%9C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uAGTRnROEL0n"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        assert d_model % num_heads == 0\n",
        "\n",
        "        self.depth = d_model // num_heads\n",
        "        self.wq = nn.Linear(d_model, d_model)\n",
        "        self.wk = nn.Linear(d_model, d_model)\n",
        "        self.wv = nn.Linear(d_model, d_model)\n",
        "        self.dense = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        x = x.view(batch_size, -1, self.num_heads, self.depth)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(self, v, k, q, mask):\n",
        "        batch_size = q.size(0)\n",
        "\n",
        "        q = self.wq(q)\n",
        "        k = self.wk(k)\n",
        "        v = self.wv(v)\n",
        "\n",
        "        q = self.split_heads(q, batch_size)\n",
        "        k = self.split_heads(k, batch_size)\n",
        "        v = self.split_heads(v, batch_size)\n",
        "\n",
        "        scaled_attention, attention_weights = self.scaled_dot_product_attention(q, k, v, mask)\n",
        "        scaled_attention = scaled_attention.permute(0, 2, 1, 3).contiguous()\n",
        "        original_size_attention = scaled_attention.view(batch_size, -1, self.d_model)\n",
        "        output = self.dense(original_size_attention)\n",
        "        return output, attention_weights\n",
        "\n",
        "    def scaled_dot_product_attention(self, q, k, v, mask):\n",
        "        matmul_qk = torch.matmul(q, k.permute(0, 1, 3, 2))\n",
        "\n",
        "        dk = torch.tensor(k.shape[-1], dtype=torch.float32)\n",
        "        scaled_attention_logits = matmul_qk / torch.sqrt(dk)\n",
        "\n",
        "        if mask is not None:\n",
        "            scaled_attention_logits += (mask * -1e9)\n",
        "\n",
        "        attention_weights = F.softmax(scaled_attention_logits, dim=-1)\n",
        "        output = torch.matmul(attention_weights, v)\n",
        "\n",
        "        return output, attention_weights\n",
        "\n",
        "class PositionwiseFeedforward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(PositionwiseFeedforward, self).__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear2(F.relu(self.linear1(x)))\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = PositionwiseFeedforward(d_model, d_ff)\n",
        "\n",
        "        self.layernorm1 = nn.LayerNorm(d_model)\n",
        "        self.layernorm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        attn_output, _ = self.mha(x, x, x, mask)\n",
        "        attn_output = self.dropout1(attn_output)\n",
        "        out1 = self.layernorm1(x + attn_output)\n",
        "\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "        return out2\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, num_layers, d_model, num_heads, d_ff, input_vocab_size, maximum_position_encoding, dropout=0.1):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(input_vocab_size, d_model)\n",
        "        self.pos_encoding = self.positional_encoding(maximum_position_encoding, d_model)\n",
        "\n",
        "        self.enc_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def positional_encoding(self, position, d_model):\n",
        "      angle_rads = self.get_angles(torch.arange(position).unsqueeze(1), torch.arange(d_model).unsqueeze(0), d_model)\n",
        "      angle_rads[:, 0::2] = torch.sin(angle_rads[:, 0::2])\n",
        "      angle_rads[:, 1::2] = torch.cos(angle_rads[:, 1::2])\n",
        "      pos_encoding = angle_rads.unsqueeze(0).transpose(0, 1)\n",
        "      return pos_encoding\n",
        "\n",
        "    def get_angles(self, pos, i, d_model):\n",
        "        angle_rates = 1 / torch.pow(10000, (2 * (i // 2)) / torch.tensor(d_model, dtype=torch.float32))\n",
        "        return pos * angle_rates\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "      seq_len = x.size(1)\n",
        "      x = self.embedding(x)\n",
        "      x *= torch.sqrt(torch.tensor(self.d_model, dtype=torch.float32)).to(x.device)\n",
        "      pos_encoding = self.pos_encoding[:seq_len, :].to(x.device)\n",
        "      x += pos_encoding.transpose(0, 1)  # Ensure pos_encoding is correctly shaped\n",
        "      x = self.dropout(x)\n",
        "\n",
        "      for i in range(self.num_layers):\n",
        "          x = self.enc_layers[i](x, mask)\n",
        "\n",
        "      return x\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, num_layers, d_model, num_heads, d_ff, input_vocab_size, target_vocab_size, pe_input, pe_target, dropout=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(num_layers, d_model, num_heads, d_ff, input_vocab_size, pe_input, dropout)\n",
        "\n",
        "        self.final_layer = nn.Linear(d_model, target_vocab_size)\n",
        "\n",
        "    def forward(self, inp, tar, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
        "      enc_output = self.encoder(inp, enc_padding_mask)\n",
        "\n",
        "      tar_seq_len = tar.size(1)\n",
        "      enc_output = enc_output[:, :tar_seq_len, :]\n",
        "\n",
        "      final_output = self.final_layer(enc_output)\n",
        "\n",
        "      return final_output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# 데이터셋 로드 및 전처리\n",
        "data_path = '/content/drive/MyDrive/ML/transformer 번역기/1_구어체(1).xlsx'\n",
        "df = pd.read_excel(data_path)\n",
        "\n"
      ],
      "metadata": {
        "id": "tjwDYdRWJhdx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, dataframe, src_tokenizer, tgt_tokenizer):\n",
        "        self.dataframe = dataframe\n",
        "        self.src_tokenizer = src_tokenizer\n",
        "        self.tgt_tokenizer = tgt_tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src_text = self.dataframe.iloc[idx, 1]  # '원문' 열\n",
        "        tgt_text = self.dataframe.iloc[idx, 2]  # '번역문' 열\n",
        "        src_tensor = torch.tensor(self.src_tokenizer.encode(src_text), dtype=torch.long)\n",
        "        tgt_tensor = torch.tensor(self.tgt_tokenizer.encode(tgt_text), dtype=torch.long)\n",
        "        return src_tensor, tgt_tensor\n",
        "\n",
        "# 샘플 토크나이저 구현\n",
        "class SimpleTokenizer:\n",
        "    def __init__(self, vocab):\n",
        "        self.vocab = vocab\n",
        "        self.word2idx = {w: i for i, w in enumerate(vocab)}\n",
        "        self.idx2word = {i: w for i, w in enumerate(vocab)}\n",
        "\n",
        "    def encode(self, text):\n",
        "        return [self.word2idx[word] if word in self.word2idx else self.word2idx['<unk>'] for word in text.split(' ')]\n",
        "\n",
        "    def decode(self, tokens):\n",
        "        return ' '.join([self.idx2word[token] for token in tokens if token in self.idx2word])\n",
        "\n",
        "# 토크나이저 초기화\n",
        "src_vocab = ['<pad>', '<sos>', '<eos>', '<unk>'] + sorted(set(' '.join(df['원문']).split()))\n",
        "tgt_vocab = ['<pad>', '<sos>', '<eos>', '<unk>'] + sorted(set(' '.join(df['번역문']).split()))\n",
        "\n",
        "src_tokenizer = SimpleTokenizer(src_vocab)\n",
        "tgt_tokenizer = SimpleTokenizer(tgt_vocab)\n",
        "\n",
        "# 데이터셋 및 데이터로더 생성\n",
        "dataset = TranslationDataset(df, src_tokenizer, tgt_tokenizer)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=lambda x: (torch.nn.utils.rnn.pad_sequence([item[0] for item in x], batch_first=True, padding_value=0),\n",
        "                                                                                   torch.nn.utils.rnn.pad_sequence([item[1] for item in x], batch_first=True, padding_value=0)))\n",
        "\n",
        "# 학습을 위한 설정\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = Transformer(num_layers=4, d_model=128, num_heads=8, d_ff=512, input_vocab_size=len(src_vocab), target_vocab_size=len(tgt_vocab), pe_input=5000, pe_target=5000).to(device)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# 학습 루프\n",
        "def create_padding_mask(seq):\n",
        "    seq = torch.eq(seq, 0).float()\n",
        "    return seq[:, None, None, :]  # (batch_size, 1, 1, seq_len)\n",
        "\n",
        "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for src, tgt in dataloader:\n",
        "        src, tgt = src.to(device), tgt.to(device)\n",
        "\n",
        "        enc_padding_mask = create_padding_mask(src)\n",
        "        look_ahead_mask = None\n",
        "        dec_padding_mask = create_padding_mask(tgt)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(src, tgt, enc_padding_mask, look_ahead_mask, dec_padding_mask)\n",
        "\n",
        "        output = output.view(-1, output.shape[-1])\n",
        "        tgt = tgt.view(-1)\n",
        "\n",
        "        min_length = min(output.size(0), tgt.size(0))\n",
        "        output = output[:min_length]\n",
        "        tgt = tgt[:min_length]\n",
        "\n",
        "        loss = criterion(output, tgt)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    loss = train_epoch(model, dataloader, criterion, optimizer, device)\n",
        "    print(f'Epoch {epoch+1}, Loss: {loss}')\n",
        "\n",
        "print(\"Training complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pH5g0zuALirc",
        "outputId": "2ff0d267-d88e-45d7-caaa-6bd249ce8158"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 7.367151946105957\n",
            "Epoch 2, Loss: 7.286356198425293\n",
            "Epoch 3, Loss: 7.277790290374756\n",
            "Epoch 4, Loss: 7.2730036095428465\n",
            "Epoch 5, Loss: 7.272238734436035\n",
            "Epoch 6, Loss: 7.27068165435791\n",
            "Epoch 7, Loss: 7.271549900817871\n",
            "Epoch 8, Loss: 7.269747857666015\n",
            "Epoch 9, Loss: 7.269885456390381\n",
            "Epoch 10, Loss: 7.2706543907928465\n",
            "Training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_sentence(model, sentence, src_tokenizer, tgt_tokenizer, device):\n",
        "    model.eval()\n",
        "    src_tensor = torch.tensor([src_tokenizer.encode(sentence)], dtype=torch.long).to(device)\n",
        "    enc_padding_mask = create_padding_mask(src_tensor)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        enc_output = model.encoder(src_tensor, enc_padding_mask)\n",
        "        translated_output = model.final_layer(enc_output)\n",
        "        translated_tokens = translated_output.argmax(dim=-1).squeeze().tolist()\n",
        "        translated_text = tgt_tokenizer.decode(translated_tokens)\n",
        "\n",
        "    return translated_text\n",
        "\n",
        "example_sentence = \"카이스트 보내주세요 제발 ㅠ\"\n",
        "translated_text = translate_sentence(model, example_sentence, src_tokenizer, tgt_tokenizer, device)\n",
        "print(f\"Translated text: {translated_text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXAieJAdRbKe",
        "outputId": "05969686-1932-46a3-80e5-56b35aacfe02"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated text: the the the the\n"
          ]
        }
      ]
    }
  ]
}